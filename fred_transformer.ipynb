{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fred/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F  \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import Transformer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pickle\n",
    "with open(\"./data/WN18RR_torch/with_5_negative_samples_data.pickle\", \"rb\") as fp:   # Unpickling\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "with open(\"./data/WN18RR_torch/with_5_negative_samples_labels.pickle\", \"rb\") as fp:   # Unpickling\n",
    "    labels = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 9917, 200, 1001, 300, 35799, 400]\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_TOKENS = ['[CLS]', '[SEP1]', '[SEP2]', '[END]']\n",
    "str_to_token_int_map = {'[CLS]':100, '[SEP1]':200, '[SEP2]':300, '[END]':400} # Special tokens first, ...\n",
    "\n",
    "def create_vocabulary(data_file):\n",
    "    \"\"\"Creates a vocabulary (index mapping) from your dataset file.\"\"\"\n",
    "    all_ids = set()\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            triple = line.strip().split(\" \")\n",
    "            subject, relation, object = triple[1], triple[3], triple[5]\n",
    "            ids = [int(subject), int(relation),int(object)]\n",
    "            all_ids.update(ids)  \n",
    "\n",
    "    # ... then numeric IDs\n",
    "    # str_to_token_int_map.update({str(token): token for token in all_ids})\n",
    "    str_to_token_int_map.update({str(token): index+1000 for index, token in enumerate(all_ids)})\n",
    "    return str_to_token_int_map\n",
    "\n",
    "def tokenize_triple(triple_str, str_to_token_int_map=str_to_token_int_map):\n",
    "    \"\"\"Tokenizes a single triple using the provided index mapping.\"\"\"\n",
    "    tokens = triple_str.strip().split(\" \")\n",
    "    numerical_ids = [str_to_token_int_map[token] for token in tokens]\n",
    "    return numerical_ids\n",
    "\n",
    "# Example Usage\n",
    "data_file = './data/WN18RR_torch/sequences/train_seq.txt'\n",
    "index_map = create_vocabulary(data_file)\n",
    "\n",
    "triple_str = '[CLS] 13266892 [SEP1] 1 [SEP2] 8107499 [END]'\n",
    "tokenized_triple = tokenize_triple(triple_str, index_map)\n",
    "print(tokenized_triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch, max_length, pad_token_id=0):\n",
    "    padded_batch = []\n",
    "    for sequence in batch:  \n",
    "        padded_seq = F.pad(sequence, (0, max_length - len(sequence)), value=pad_token_id) \n",
    "        padded_batch.append(padded_seq)\n",
    "    return torch.stack(padded_batch)  \n",
    "\n",
    "def create_src_mask(padded_batch, pad_token_id=0):\n",
    "    return (padded_batch == pad_token_id).int()\n",
    "\n",
    "def collate_fn(batch):  # Custom collate function for DataLoader\n",
    "    inputs, targets = zip(*batch)  # Unpack example pairs\n",
    "    max_length = max(len(seq) for seq in inputs) \n",
    "    padded_inputs = pad_batch(inputs, max_length)\n",
    "    src_mask = create_src_mask(padded_inputs)\n",
    "    return padded_inputs, targets, src_mask\n",
    "\n",
    "tokenized_data = torch.tensor(list(map(tokenize_triple, data)), dtype=torch.int32).to(device)\n",
    "labels = torch.tensor(labels).reshape(len(labels),1).to(device)\n",
    "dataset = TensorDataset(tokenized_data, labels)  # Create a PyTorch Dataset\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1018"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerEncoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, num_heads, \n",
    "#                  num_encoder_layers, feedforward_dim, dropout=0.1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.encoder_layers = nn.ModuleList([\n",
    "#             TransformerEncoderLayer(embedding_dim, num_heads, feedforward_dim, dropout) \n",
    "#             for _ in range(num_encoder_layers)\n",
    "#         ])\n",
    "#         self.output_layer = nn.Linear(embedding_dim, 1)  # For score output\n",
    "\n",
    "#     def forward(self, src_tokens, src_mask=None): \n",
    "#         embedded = self.embedding(src_tokens) \n",
    "\n",
    "#         for layer in self.encoder_layers:\n",
    "#             embedded = layer(embedded,src_mask) \n",
    "\n",
    "#         #score prediction\n",
    "#             print(self.output_layer)\n",
    "#         score = self.output_layer(embedded[:, 0, :]) \n",
    "#         return score.squeeze()  # Output a single score\n",
    "\n",
    "# class TransformerEncoderLayer(nn.Module):\n",
    "#     def __init__(self, embedding_dim, num_heads, feedforward_dim, dropout):\n",
    "#         super().__init__()\n",
    "#         self.self_attn = nn.MultiheadAttention(embedding_dim, num_heads, dropout)\n",
    "#         self.ffn = nn.Sequential(\n",
    "#             nn.Linear(embedding_dim, feedforward_dim),  \n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(feedforward_dim, embedding_dim)\n",
    "#         )\n",
    "#         # LayerNorm and dropout would typically be here, omitted for brevity \n",
    "    \n",
    "#     def forward(self, src, src_mask=None):\n",
    "#         # Self-attention\n",
    "#         attn_output, _ = self.self_attn(src, src, src, key_padding_mask=src_mask)\n",
    "#         # Feed-forward\n",
    "#         ffn_output = self.ffn(attn_output) \n",
    "#         return ffn_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40574\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(set(tokenized_data.flatten().tolist()))\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary_size = len(set(tokenized_data.flatten().tolist()))\n",
    "# embedding_dim = 16\n",
    "# num_heads = 8\n",
    "# num_encoder_layers = 6\n",
    "# feedforward_dim = 1024\n",
    "# dropout=0.1\n",
    "\n",
    "\n",
    "# model = TransformerEncoder(vocabulary_size,embedding_dim,num_heads,num_encoder_layers,feedforward_dim,dropout).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     for i,(input,target_scores,src_mask) in enumerate(dataloader):\n",
    "#        print(i)\n",
    "#        optimizer.zero_grad()\n",
    "#        outputs = model(input,src_mask)  \n",
    "#        loss = loss_fn(outputs, target_scores)  \n",
    "#        loss.backward()\n",
    "#        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SequenceValidityModel(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(SequenceValidityModel, self).__init__()\n",
    "        self.transformer = Transformer(d_model, nhead, num_layers, dim_feedforward)\n",
    "        self.linear = nn.Linear(d_model, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, src):\n",
    "        output = self.transformer(src)\n",
    "        output = self.linear(output[-1, :, :])  # Use the last output for classification\n",
    "        return output\n",
    "\n",
    "# Initialize the model\n",
    "model = SequenceValidityModel(d_model=16, nhead=4, num_layers=2, dim_feedforward=128)\n",
    "\n",
    "# Define a loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "# for epoch in tqdm(range(10)):\n",
    "#     for i, (inputs, labels,src_mask) in enumerate(dataloader):\n",
    "#         print(i)\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs)\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "#         print(epoch,' ',loss)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
